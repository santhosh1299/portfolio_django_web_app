{% extends 'portfolio/base.html' %}

{% block content %}
{% load static %}
<script>
    document.addEventListener('DOMContentLoaded', function () {
        const scrollableNavLinks = document.querySelectorAll('.scrollable-nav .side-heading');
        
        scrollableNavLinks.forEach(function (link) {
            link.addEventListener('click', function (e) {
                e.preventDefault();
                
                const targetId = this.getAttribute('href').substring(1);
                const targetSection = document.getElementById(targetId);
                
                if (targetSection) {
                    window.scrollTo({
                        top: targetSection.offsetTop - 70, // Adjust the offset as needed
                        behavior: 'smooth'
                    });
                }
            });
        });
    });
</script>

<style>
    body {
        margin: 0;
        font-family: 'Lato', sans-serif;
        background-color: #f0f0f0;
        overflow-x: hidden;
    }

nav {
    background-color: #343a40;
    padding: 15px;
    border-radius: 10px;
    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
}

.navbar-brand {
    color: #fff;
    font-weight: bold;
}

.navbar-toggler-icon {
    background-color: #fff;
}

.navbar-toggler {
    border: none;
}

.navbar-nav .nav-link {
    color: #fff;
    font-weight: bold;
}

.navbar-nav .nav-link:hover {
    color: #ffcc00; /* Change to your desired hover color */
}


   .side-heading {
        color: #343a40;
        text-decoration: none;
        font-weight: normal;
        padding: 13px;
        background-color: #f8f9fa;
        border-radius: 5px;
        cursor: pointer;
        transition: background-color 0.3s ease, color 0.3s ease, transform 0.3s ease;
    }

    .side-heading:hover {
        background-color: #343a40;
        color: #fff;
    }

    .side-heading.active {
        font-weight: bold;
        color: #fff;
        background-color: #555;
    }

   section {
    padding: 30px;
    text-align: left;
    margin: 30px 0;
    background-color: #fff;
    box-shadow: 0 0 15px rgba(0, 0, 0, 0.1);
    border-radius: 15px;
    position: relative; /* Add this line to make room for the fixed header */
    top: 70px; /* Add this line to push the section down below the fixed header */
}

/* Updated Section Heading Styles */
section h2 {
    font-weight: bold;
    color: #343a40;
    font-size: 2em;
    margin-bottom: 20px;
}
section h2 {
    font-size: 2.2em; /* Increase the font size for section headings */
    margin-bottom: 15px; /* Add more spacing between headings and paragraphs */
}

/* Updated Section Paragraph Styles */
section p {
    color: #555;
    line-height: 1.6;
    text-align: justify;
}

/* Updated Image Styles */
section img {
    max-width: 100%;
    height: auto;
    align-self: center;
    border-radius: 10px;
    box-shadow: 0 0 15px rgba(0, 0, 0, 0.1);
}
    .scrollable-nav {
        position: fixed;
        top: 0;
        left: 0;
        width: 200px;
        background-color: transparent;
        display: flex;
        flex-direction: column;
        align-items:stretch;
        padding-top: 60px;
        box-shadow: -2px 0 5px rgba(0, 0, 0, 0.1);
        z-index: 2;
        overflow-y: auto;
    }
article {
        margin-left: 220px; 
        width: calc(100% - 220px); 
    }

.scrollable-nav .side-heading {
    
    text-decoration: none;
    font-weight: normal;
    padding: 13px;
    border-radius: 5px;
    cursor: pointer;
    transition: background-color 0.3s ease, color 0.3s ease;
}

.scrollable-nav .side-heading:hover {
    background-color: #555;
}

h2.title {
        font-size: 3em; /* Adjust the font size as needed */
        font-weight: bold;
        color: #333; /* Set the color as needed */
        text-align: center; /* Center the text */
        margin-bottom: 20px; /* Add margin at the bottom for spacing */
    }
/* Additional styles for the #sectionTitle */
section#sectionTitle {
    background-color: #343a40; /* Set the background color as needed */
    padding: 30px; /* Adjust padding as needed */
    border-radius: 10px; /* Add border-radius for rounded corners */
    border: 2px solid #fff; /* Add a border with a white color */
    box-shadow: 0 0 15px rgba(0, 0, 0, 0.2); /* Add a subtle box shadow */
}

/* Adjust the font size, weight, and color of the title within #sectionTitle */
section#sectionTitle h2.title {
    font-size: 2.5em; /* Adjust the font size as needed */
    font-weight: bold; /* Adjust the font weight as needed */
    color: #fff; /* Set the text color as needed */
    text-align: center; /* Center the text */
    margin-bottom: 20px; /* Add margin at the bottom for spacing */
    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2); /* Add a subtle text shadow */
}

p {
            font-size: 1.2em;
            line-height: 1.6em;
        }

li 
{
            font-size: 1.2em;
            line-height: 1.6em;
        }
        img {
    max-width: 100%;
    height: auto;
    border-radius: 10px;
    box-shadow: 0 0 15px rgba(0, 0, 0, 0.2);
    display: block;
    margin: auto; /* Add margin for better separation */
}
p a {
    color: #555;
    display: block;
    text-align: center;
    margin: auto;
    transition: color 0.3s ease, text-decoration 0.3s ease;
}

p a:hover {
    color: #333;
    text-decoration: underline;
}
article p a {
    color: #555;
    display: block;
    text-align: center;
    margin: auto;
    transition: color 0.3s ease, text-decoration 0.3s ease;
}

article p a:hover {
    color: #333;
    text-decoration: underline;
}
h4 {
    font-size: 1.2em; /* You can adjust the value as needed */
    font-weight: bold;
}

/* Increase h3 size and make it bolder */
h3 {
    font-size: 1.4em; /* You can adjust the value as needed */
    font-weight: bold;
}
    
.indented {
        display: inline-block;
        margin-left: 40px; /* Adjust as needed */
    }
</style>

<nav class="scrollable-nav">
    
    <a class="side-heading" href="#section1">Introduction</a>
    <a class="side-heading" href="#section2">Data Prep_EDA</a>
    <a class="side-heading" href="#section3">Clustering</a>
    <a class="side-heading" href="#section4">ARM</a>
    <a class="side-heading" href="#section5">NaiveBayes</a>
    <a class="side-heading" href="#section6">DecTrees</a>
    <a class="side-heading" href="#section7">SVMs</a>
    <a class="side-heading" href="#section10">Conclusion</a>
</nav>
<article>
<section  id="sectionTitle"  style=" padding-top: 70px;" >
    <h2 class="title" >Lithium Ion Battery Discharge Capacity Prediction <br/>And<br/> Analysis of Charge Cycling</h2>
</section>

<section id="section1" >
    <h2 class ="section-heading" >Introduction</h2>
    <p>
        The Growing urgency of Climate change has led to electrification which has led to an increase in the usage of battery technologies, mainly Lithium-ion battery is predominantly used for many reasons. They possess high energy density which allows them to store more energy in relatively small sizes. Due to this property, it is widely used in portable electronic devices. Long-cycle capacity allows these types of batteries to provide electrical support for longer periods. Mainly, Lithium-ion batteries are environmentally friendly compared to other batteries where these batteries can be recycled using advanced recycling techniques, with a reduced environmental impact. These batteries are virtually low in maintenance, making them suitable for many applications and user-friendly. Their versatility in terms of designs makes it possible to be used in small devices like calculators to battery-powered electric cars, it is available in various designs and sizes including coin cells, pouch cells, and cylindrical cells. Nowadays smartphones require more voltage and in comparison with other rechargeable batteries lithium-ion provides high voltage per cell.   </p>
    <img src={% static 'portfolio/battery-header.jpg' %} alt="Lithium Ion Battery- header" width="750" height="465">
    <p style="font-size: 0.9em; color: #888; margin-top: 5px; transition: color 0.3s ease, text-decoration 0.3s ease;"
   onmouseover="this.style.color='#555'; this.style.textDecoration='underline';"
   onmouseout="this.style.color='#888'; this.style.textDecoration='none';">
   <a href="https://www.miningmagazine.com/technology/news/1458744/direct-lithium-extraction" target="_blank">
       Source: https://www.miningmagazine.com/technology/news/1458744/direct-lithium-extraction
   </a>
</p>

    
    <p>
        The increased dependency on Lithium-ion batteries in every field makes maintenance a crucial aspect, underscoring the potentially disastrous consequences and the heavy reliance on expensive systems for proper care. This holds particularly true for a myriad of applications, spanning from everyday devices like phones and laptops to essential components in vehicles and servers. Indeed, the significance of maintaining lithium-ion batteries cannot be overstated, as these power sources are integral to the seamless operation of our technological landscape.
    </p>
    <img src="https://ul.org/sites/default/files/inline-images/Lithium%20Ion%20Cell%203.png" alt="Lithium Ion Battery" width="750" height="465">
    <p style="font-size: 0.9em; color: #888; margin-top: 5px; transition: color 0.3s ease, text-decoration 0.3s ease;"
   onmouseover="this.style.color='#555'; this.style.textDecoration='underline';"
   onmouseout="this.style.color='#888'; this.style.textDecoration='none';">
   <a href="https://ul.org/research/electrochemical-safety/getting-started-electrochemical-safety/what-are-lithium-ion" target="_blank">
       Source: https://ul.org/research/electrochemical-safety/getting-started-electrochemical-safety/what-are-lithium-ion
   </a>
</p>

    <p>
        According to Consumer Reports' 2023 Annual Auto Reliability Survey, electric vehicles have 79% reliability issues compared to gasoline cars. Battery-related issues are higher when we look into these issues in depth. Some issues like unexpected drops in driving range and reduced battery performance can be mitigated by predictive maintenance, which is the integration of Remaining Useful Battery Life and discharge capacity prediction. It is very useful in guiding consumers to increase the lifespan and reliability of these batteries. Additionally, it can be integrated with dynamic charging algorithms to adapt charging patterns to ensure that minimizes degradation and maximizes the usable life.
    </p>

    <br>
    <p>
        There was some research done on leveraging the Deep Learning Models for battery degradation prediction. This involves analyzing the battery's health and degradation patterns. This research was done purely on MATLAB and there were some limitations on the flexibility compared to other frameworks like TensorFlow or PyTorch. Additionally, MATLAB may not be scalable for large-scale deep-learning objects. Another research was done on the Remaining Useful Life Prediction using the Stacked Autoencoder (SAE) and Gaussian Mixture Regression. This approach led to the introduction of indirect health indicators from the Voltage, Current, and Temperature data to overcome the practical limitations.  </p>   
    <p>
    <p>The intricate interplay between voltage and temperature within these batteries holds crucial implications for their overall performance and longevity. Throughout the charging and discharging cycles, voltage fluctuations serve as indicators of internal resistance changes and electrolyte behavior, offering invaluable insights into the battery's state of health. Analyzing voltage profiles across diverse charging conditions enables predictive models to accurately estimate the remaining capacity of the battery, facilitating proactive maintenance strategies to optimize its lifespan.Simultaneously, elevated temperatures present a formidable challenge, as they expedite degradation mechanisms within lithium-ion batteries, leading to capacity fade and compromised performance. By monitoring temperature variations during charging cycles, predictive maintenance strategies can effectively mitigate thermal stress and extend the battery's useful life. Moreover, the integration of voltage and temperature data from an array of charging profiles serves as the bedrock for the development of robust predictive maintenance techniques. Leveraging advanced analytics and machine learning algorithms, stakeholders can assess the profound impact of operational conditions on battery health with unprecedented accuracy.</p>

    <p>This proactive approach empowers stakeholders to implement timely interventions, such as adjusting charging protocols or deploying sophisticated thermal management strategies, to optimize battery performance and reliability. By making informed decisions grounded in comprehensive data analysis, stakeholders ensure the efficient and sustainable operation of lithium-ion battery systems across diverse applications. In essence, the seamless integration of voltage and temperature data not only enhances predictive maintenance capabilities but also lays the foundation for the continued advancement of energy storage technologies, driving innovation and sustainability in the modern era.</p>

    <p>Understanding the battery life of portable devices like smartphones and laptops holds significant importance for users and manufacturers alike. Longer battery life directly enhances user experience by allowing extended usage without frequent charging interruptions, thereby increasing convenience, productivity, and satisfaction. Additionally, battery life serves as a key differentiator among device models in a competitive market, influencing purchasing decisions and brand loyalty. Knowledge of battery life enables users to optimize device usage, conserve power, and plan activities effectively, particularly when away from charging points for extended periods. Longer battery life also contributes to environmental sustainability by reducing electronic waste and encouraging device longevity. Moreover, in emergency situations or for professionals who rely heavily on their devices for work, longer battery life can be critical for communication and task execution. Overall, battery life plays a crucial role in shaping user experiences, product competitiveness, and environmental impact in the realm of portable devices.</p>
        
    <h3>Research Questions:</h3>

        <ol style="font-size: 1.2em; line-height: 1.6em;">
            
            <li>Which Battery Groups has the widest range of values in the experiement? </li> 
            <li>Whether all the battery groups in the dataset has decay phase according to the general assumption?</li>
            <li>How the battery life is grouped and does it matches the assumption made by analysing the data?</li>
            <li>What are the recent news about batteries?</li>
            <li>Can we predict the battery life capacity based on the parameters like voltage, current and temperature of charge cycles?</li>
            <li>Compare the discharge capacity and remaining battery life in a timeline and find out patterns?</li>
            <li>Does lower ambient temperature affects the aging of the batteries?</li>
            <li>Does the discharge capacity contribute in prediction of remaining useful battery life (BL)?</li>
            <li>Does the battery parameters improve if we use proper intervals</li>
            <li>How does the discharge capacity measured at the charger vary during charge cycles for Battery group 1?</li>
          
         
        </ol>
    </p>

</section>

<section id="section2" >
    <h2>Data Prep_EDA</h2>
    <h3>Data Collection</h3>
    <p>The data has been gathered from two sources</p>
    <ol style="font-size: 1.2em; line-height: 1.6em;">
        <img src={% static 'portfolio/kaggle-logo.jpeg' %} alt="kaggle-logo" width="250" height="465">
        <br>
        <p style="font-size: 0.9em; color: #888; margin-top: 5px; transition: color 0.3s ease, text-decoration 0.3s ease;"
   onmouseover="this.style.color='#555'; this.style.textDecoration='underline';"
   onmouseout="this.style.color='#888'; this.style.textDecoration='none';">
   <h3>1. Kaggle Data</h3>
   <p>Data Link: </p>
   <a href="https://www.kaggle.com/datasets/patrickfleith/nasa-battery-dataset/data" target="_blank">
    https://Kaggle.com - https://www.kaggle.com/datasets/patrickfleith/nasa-battery-dataset/data
   </a>
   <p>Code Link: </p>
   <a href="https://github.com/santhosh1299/Project/blob/main/data_gathering.ipynb" target="_blank">
    https://github.com/santhosh1299/Project/blob/main/data_gathering.ipynb
   </a>
   <a href="https://github.com/santhosh1299/Project/blob/main/data_cleaning.ipynb" target="_blank">
    https://github.com/santhosh1299/Project/blob/main/data_cleaning.ipynb
   </a>
</p>
        
        <br>
        <p>This data set has been collected from a custom built battery prognostics testbed at the NASA Ames Prognostics Center of Excellence (PCoE). Li-ion batteries were run through 3 different operational profiles (charge, discharge and Electrochemical Impedance Spectroscopy) at different temperatures. Discharges were carried out at different current load levels until the battery voltage fell to preset voltage thresholds. Some of these thresholds were lower than that recommended by the OEM (2.7 V) in order to induce deep discharge aging effects. Repeated charge and discharge cycles result in accelerated aging of the batteries. The experiments were stopped when the batteries reached the end-of-life (EOL) criteria of 30% fade in rated capacity (from 2 Ah to 1.4 Ah).</p>
        <br>
        <p>Raw Data</p>
        <img src={% static 'portfolio/data_clean/kaggle_before.png' %} alt="api-logo" width="815" height="225">
        <br>
        <p>After Cleaning</p>
        <br>
        <img src={% static 'portfolio/data_clean/kaggle_after.png' %} alt="api-logo" width="800" height="290">
        <br>
        <p style="font-size: 0.9em; color: #888; margin-top: 5px; transition: color 0.3s ease, text-decoration 0.3s ease;"
   onmouseover="this.style.color='#555'; this.style.textDecoration='underline';"
   onmouseout="this.style.color='#888'; this.style.textDecoration='none';">

   <h3>2. API Source : </h3>
   <a href="https://open-meteo.com/" target="_blank">
    https://open-meteo.com/ - Historical Weather Data API
   </a>
</p>
        
        <p style="font-size: 0.9em; color: #888; margin-top: 5px; transition: color 0.3s ease, text-decoration 0.3s ease;"
   onmouseover="this.style.color='#555'; this.style.textDecoration='underline';"
   onmouseout="this.style.color='#888'; this.style.textDecoration='none';">
   <p>Endpoint : </p>
   <a href="https://api.open-meteo.com/v1/forecast" target="_blank">
    "https://api.open-meteo.com/v1/forecast"
   </a>
   <p>Code Link : </p>
   <a href="https://github.com/santhosh1299/Project/blob/main/api_data.ipynb" target="_blank">
    "https://github.com/santhosh1299/Project/blob/main/api_data.ipynb"
   </a>
</p>
        <br>
        <p>Raw Data</p>
        <img src={% static 'portfolio/data_clean/api_before.png' %} alt="api-logo" width="941" height="263">
        <br>
        <p>After Cleaning</p>
        <br>
        <img src={% static 'portfolio/data_clean/api_after.png' %} alt="api-logo" width="400" height="222">
        <br>
        
        
        <p>The historical weather data of Moutain View, California where the NASA Ames Prognostics Center of Excellence (PCoE) located for the period of two months to analyse the impact of ambient temperature in the battery aging.</p>

    </ol>

    <h3>Data Cleaning</h3>
    <ol style="font-size: 1.2em; line-height: 1.6em;">
        <br>
        <li> <h4>API Data</h4> </li>
        <br>
        <h4>Checking for Unwanted Columns</h4>
        <p> The raw data has been processed into a data frame and it contains an unwanted redundant index column. As the first step of cleaning it has been removed.</p>
        <br>
        <p>Before Cleaning</p>
        <img src={% static 'portfolio/data_clean/api_removingcols_before.png' %} alt="api-logo" width="450" height="188">
        <br>
        <p>After Cleaning</p>
        <img src={% static 'portfolio/data_clean/api_removingcols_after.png' %} alt="api-logo" width="450" height="188">
        <br>
        <h4>Checking for Incorrect Data Types</h4>
        <p>As the next step, the data is checked for data type. This dataset is in correct datatype, the Date feature as Datetime64 and temperature feature as Float</p>
        <br>
        <img src={% static 'portfolio/data_clean/api_check_datatype.png' %} alt="api-logo" width="450" height="135">
        <br>
        <h4>Checking for Null Values</h4>
        <br>
        <p>The null values are checked for this dataset using isna() function and there were no null values in this dataset.</p>
        <img src={% static 'portfolio/data_clean/api_check_null.png' %} alt="api-logo" width="400" height="161">
        <br>
        <h4>Checking for Duplicated Rows</h4>
        <br>
        <p>There were no duplicated rows in this dataset</p>
        <img src={% static 'portfolio/data_clean/api_duplicate.png' %} alt="api-logo" width="450" height="74">
        <br>
        <h4>Changing the month to match battery dataset</h4>
        <br>
        <p>Due to data unavailability, in this API. The month values are changed from September to April and October to May has been modified to analyse the ambient weather and analyse the changes in other parameters.</p>
        <img src={% static 'portfolio/data_clean/api_manipulation.png' %} alt="api-logo" width="450" height="263">
        <br>
        <br>
        <li> <h4>Kaggle Data</h4> </li>
        <br>
        <h4>Checking for Unwanted Columns</h4>
        <br>
        <p>The raw data contains an unwanted columns. As the first step of cleaning it has been removed.</p>
        <img src={% static 'portfolio/data_clean/kaggle_removingcols.png' %} alt="api-logo" width="450" height="250">
        <br>
        <h4>Checking for Incorrect Data Types</h4>
        <br>
        <p>As the next step, the data is checked for data type. This dataset has 7 features, the Capacity feature as object type was converted to float and other features are in correct data type.</p>
        <br>
        <p>Before Cleaning</p>
        <img src={% static 'portfolio/data_clean/kaggle_check_datatypes_before.png' %} alt="api-logo" width="450" height="209">
        <br>
        <p>After Cleaning</p>
        <img src={% static 'portfolio/data_clean/kaggle_check_datatypes_after.png' %} alt="api-logo" width="450" height="359">
        <br>
        <h4>Checking for Null Values</h4>
        <br>
        <p>The null values are checked for this dataset using isna() function and there were no null values in this dataset.</p>
        <img src={% static 'portfolio/data_clean/kaggle_missing_value.png' %} alt="api-logo" width="450" height="263">
        <br>
        <h4>Merging Metadata with aggregated experiment data</h4>
        <br>
        <p> The Experiment metadata table has been merged with the aggregated values from the experiment data. Mean, median and standard deviation of the Voltage measured, Current measured, Voltage load, Current Load and Time.</p>
        <img src={% static 'portfolio/data_clean/kaggle_discharge_merge_data.png' %} alt="api-logo" width="941" height="263">
        <br>
        <h4>Checking missing values in merged data</h4>
        <br>
        <p>The merged data is checked for missing values and there were no missing values.</p>
        <img src={% static 'portfolio/data_clean/kaggle_merge_missing_value.png' %} alt="api-logo" width="450" height="720">
        <br>
    <h3>Exploratory Data Analysis</h3>
    <ol style="font-size: 1.2em; line-height: 1.6em;">
        <br>

        <h4>Remaining Useful Battery Life(Minutes) of All Battery Groups</h4>
        <br>
        <p>The median of battery life among all the different battery groups. The battery group 5 has the highest and battery group 3 has the lowest.</p>
        <img src={% static 'portfolio/eda/1.png' %} alt="api-logo" width="941" height="263">
         <br>
        <h4>Remaining Useful Battery Life(Minutes) Distribution Analysis</h4>
        <br>
        <p>The battery groups 5 and 6 have values from both extreme values. This indicates that there must be a gap during the experiment.</p>
        <img src={% static 'portfolio/eda/2.png' %} alt="api-logo" width="941" height="263">
        <br>
        <h4>Trend Analysis of Remaining Useful Battery Life(Minutes) During The Experiment</h4>
        <br>
        <p>Analysing the trend of the median Remaining Useful Battery Life(Minutes), it is observed from the visualisation the group 5 and 6 have growing trend</p>
        <img src={% static 'portfolio/eda/3.png' %} alt="api-logo" width="941" height="263">
        <br>
        <h4>Indepth Analysis on Group 5 and 6</h4>
        <br>
        <p>On indepth analysis of battery group 5 and 6 it is confirmed that there was a gap in both experiment groups which is the main reason for the increase in the median remaining useful life.</p>
        <img src={% static 'portfolio/eda/4.png' %} alt="api-logo" width="941" height="263">
        <br> 
        <h4>Clustering Battery Groups on Discharge Capacity and Remaining Useful Battery Life(Minutes)</h4>
        <br>
        <p>On clustering, it can be observed that the battery life and discharge capacity are correlated to eachother.</p>
        <img src={% static 'portfolio/eda/5.png' %} alt="api-logo" width="941" height="263">
        <br>
        <h4>Discharge Capacity Distribution Analysis</h4>
        <br>
        <p>The battery groups 4 and 8 have values from both extreme values. This indicates that it needs an further analysis to find more about the discharge </p>
        <img src={% static 'portfolio/eda/6.png' %} alt="api-logo" width="941" height="263">
        <br>
        <h4>Comparison of Discharge Capacity with that of Remaining Useful Battery Life (Minutes) of Battery #33</h4>
        <br>
        <p>One interesting dip has been observed in the discharge capacity, it must be an outlier or some interesting pattern which needed to explored to find the reason about the dip.</p>
        <img src={% static 'portfolio/eda/7.png' %} alt="api-logo" width="941" height="263">
        <br>
        <h4>Histogram of temperature</h4>
        <br>
        <p>This histogram indicates that some batteries have lower temperatures and some have higher temperatures. </p>
        <img src={% static 'portfolio/eda/8.png' %} alt="api-logo" width="941" height="263">
        <br>
        <h4>Comparison of Temperature with Current Load and Voltage Load </h4>
        <br>
        <p>It can be observed the correlation between the current load and the temperature</p>
        <img src={% static 'portfolio/eda/9.png' %} alt="api-logo" width="941" height="263">
        <br>
        <h4>Temperature vs Remaining Useful Battery Life(Minutes)</h4>
        <br>
        <p>Another interesting observation about the battery group 7 which has higher median battery life within 10deg celsius</p>
        <img src={% static 'portfolio/eda/10.png' %} alt="api-logo" width="941" height="263">
        <br>
        
        



</section>


<section id="section3">
    <h2>Clustering</h2>

    <h3>Overview</h3>
    <p>Clustering is like the pivotal element of unsupervised learning, it helps in bringing together data points based on their natural similarities, in multiple dimensions. It does not need labelled data as required by the supervised learning. Instead, it dives into the data, searching for hidden structures without any guidance or context of the data. As it sifts through the dataset, clusters start to form, like groups of data points, each sharing common patterns or behaviors. 
        These clusters shed light on the different personalities within the data, revealing patterns and connections we might not have noticed before. It is usually used in research where the labels are created by clustering and it helps in providing more context abt the data from the inner lying patterns
        </p>

        <p>In the context of predicting discharge capacity in lithium-ion batteries, clustering techniques can be leveraged to identify thresholds for data discretization. By analyzing features related to battery performance, such as voltage, temperature, and charging profiles, clustering algorithms can effectively group similar data points together based on their intrinsic characteristics. This clustering information provides a nuanced understanding of the dataset's structure, highlighting distinct clusters that exhibit varying levels of correlation with discharge capacity.</p>
        <p>Moreover, by excluding the discharge capacity feature from the clustering process, we can focus solely on identifying patterns in other relevant features that may indirectly influence discharge capacity. Through this approach, clusters with similar characteristics to discharge capacity can be identified, providing valuable insights into the factors that contribute to battery performance. Subsequently, these clustering results can inform the data discretization process, guiding the selection of appropriate thresholds for binning continuous variables into discrete categories.</p>
        <p>By incorporating clustering information into the data discretization process, we can ensure that the resulting discretized features capture meaningful variations in battery performance while minimizing information loss. This iterative approach enhances the interpretability and predictive power of subsequent models built upon the discretized data. Ultimately, leveraging clustering algorithms in this manner enables us to extract actionable insights from the dataset and optimize the predictive accuracy of discharge capacity models in lithium-ion battery systems.</p>

    
    <h4>Types</h4>

    <h4>Partitional Clustering</h4>
    <br>
     
      <img src={% static 'portfolio/clustering/partitional_clust.png' %} alt="api-logo" width="500" height="483">
      <br>
    <ul>
        <li>
          It divides the dataset into non-overlapping groups based on their distance from one another. It is suitable for large datasets as it is scalable and efficient.
        </li>
        <li>
          The cluster numbers are specified by the analyst, and there are ways to find an optimal number of clusters best for the data when there is no idea about the actual context of the data.
        </li>
        <li>
          It includes methods like K-means and K-modes.
        </li>
      </ul>
    <h4>KMeans</h4>
    <p>In K-means clustering, the algorithm partitions the data into K clusters by iteratively assigning each data point to the nearest cluster centroid and updating the centroids based on the mean of the points in each cluster. K-means is widely used due to its simplicity and efficiency, but it requires specifying the number of clusters beforehand.</p>

   

    <h4>Hierachical Clustering</h4>

    <br>
     
      <img src={% static 'portfolio/clustering/heirar_clust.png' %} alt="api-logo" width="500" height="395">
      <br>
    <p>Hierarchical clustering is a technique that creates a hierarchical tree-like structure of clusters, where each node represents a cluster. This method does not require the analyst to specify the number of clusters beforehand and is particularly useful when the dataset's structure is not well understood.
    </p>
    <h4>Agglomerative  Clustering</h4>
    <ul>
        <li>
          Agglomerative clustering, also known as bottom-up clustering, starts with each data point as a separate cluster. It iteratively merges the closest clusters until a single cluster containing all data points is formed.
        </li>
        <li>
          It considers each cluster as a single cluster and merges two clusters as one until there is only one cluster remaining.
        </li>
      </ul>

    <h4>Linking methods</h4>
    <ul>
        <li>
          Single Linkage: Merge the clusters based on the smallest distance between any points in the two clusters.
        </li>
        <li>
          Complete Linkage: Merge the clusters based on the maximum distance between any points in the two clusters.
        </li>
        <li>
          Average Linkage: Merge the clusters based on the average distance between any points in the two clusters.
        </li>
      </ul>
      

    <h4>Divisive  Clustering</h4>
    <ul>
        <li>
          Divisive clustering, also known as top-down clustering, starts with all data points in a single cluster and recursively splits clusters into smaller clusters until each cluster contains only one data point.
        </li>
        <li>
          It is the opposite of Agglomerative clustering. It clusters the data by considering all data points as one and splits them until each data point becomes a cluster. The splitting on each iteration is based on two criteria: intercluster dissimilarity and intercluster similarity.
        </li>
      </ul>
      
     <h3>Commonly Used Distance Metrics</h3>
     <h4>Euclidean Distance</h4>
<p>
  Perhaps the most widely used distance metric, Euclidean distance measures the straight-line distance between two data points in Euclidean space. It is calculated as the square root of the sum of squared differences between corresponding coordinates of two points. Euclidean distance is suitable for continuous numeric data and assumes that the data points are distributed in a continuous space.
</p>

<h4>Manhattan Distance</h4>
<p>
  Also known as city block distance or taxicab distance, Manhattan distance measures the sum of the absolute differences between corresponding coordinates of two points. It represents the distance traveled along axes at right angles. Manhattan distance is suitable for data with attributes measured on different scales or for which the concept of straight-line distance may not be meaningful.
</p>

<h4>Cosine Similarity</h4>
<p>
  Unlike Euclidean and Manhattan distances, cosine similarity measures the cosine of the angle between two vectors, representing the similarity in direction rather than magnitude. It is commonly used for text data, where documents are represented as vectors of word frequencies or TF-IDF scores. Cosine similarity is robust to differences in vector magnitudes and is effective for measuring similarity between sparse vectors.
</p>


    <h3>Density-based Clustering</h3>

    <br>
     
      <img src={% static 'portfolio/clustering/density_based.png' %} alt="api-logo" width="500" height="493">
      <br>
    <ul>
        <li>
          Density-based clustering is a type of clustering algorithm that identifies clusters based on the density of data points in the feature space.
        </li>
        <li>
          Unlike partitioning or hierarchical clustering, which assume that clusters have well-defined boundaries, density-based clustering methods can discover clusters of arbitrary shapes and sizes.
        </li>
        <li>
          One popular density-based clustering algorithm is DBSCAN (Density-Based Spatial Clustering of Applications with Noise).
        </li>
        <li>
          The main advantages of density-based clustering are its ability to handle clusters of arbitrary shapes and sizes and its robustness to noise and outliers.
        </li>
      </ul>
      
      <h3>Data Prep</h3>
      <br><br>
      <div>
        <span class="indented"> <h4>Code : <a href="https://github.com/santhosh1299/Project/blob/main/API_news.ipynb">News data collection from API</a></h4> </span>
    </div>
<br>
      <p>Clustering algorithms involve mathematical operations such as calculating centroids, updating cluster assignments, or determining cluster distances. These operations are more straightforward to perform on numeric data, as mathematical operations are well-defined for numerical values. Handling categorical or text data requires additional preprocessing steps, such as one-hot encoding or feature scaling, to convert them into numeric format.
      </p>
      <br>
      <h4>Before</h4>
      <br>
      <p>There are both qualitative and quantitative data types are present in the data</p>
      <img src={% static 'portfolio/clustering/data_prep_before.png' %} alt="api-logo" width="941" height="263">
      <br>
      <br>
      <h4>After</h4>
      <br>
      <p>It only contains quantitative data and it is ready for clustering</p>
      <img src={% static 'portfolio/clustering/data_prep_after.png' %} alt="api-logo" width="941" height="263">
      <br>
      <br>
      
      <h3>Code</h3>
      <br><br>
      <h4>Kmeans</h4>
      <br>
      <div>
        <span class="indented"><h4>Code : <a href="https://github.com/santhosh1299/Project/blob/main/Clustering-kmeans.ipynb">Kmeans in Python</a></h4>  </span>
      </div>
     
   <br><br>

      <h4>Hierachical Clustering</h4>
     <br> 
      <div>
        <span class="indented"><h4>Code : <a href="https://github.com/santhosh1299/Project/blob/main/Clustering-hclust.ipynb">Hclust in R</a></h4></span>
    </div>
<br><br>
     

      <h3>Results</h3>
      <br>
<p>In the course of our analysis, particular emphasis was placed on the pivotal role of voltage and battery attributes. To illuminate our findings, we employed K-means clustering to discern distinct patterns within our dataset. Leveraging the resulting clusters, we embarked on a process of discretizing our output variables, notably the Remaining Useful Battery Life (RUL) and discharge capacity. With a silhouette score of 11 indicating well-defined clusters, we juxtaposed the clustering outcomes alongside the battery life and discharge capacity values in a scatter plot. This strategic visualization allowed for a comprehensive examination of how these values are clustered and their interrelation within the broader dataset. By integrating clustering results with output variable discretization, we have gained valuable insights into the intricate relationships between input and output variables. This approach not only facilitates the identification of underlying patterns but also enhances our understanding of how these variables contribute to the overall clustering structure, thereby enriching the depth of our analysis and guiding informed decision-making processes.</p>

        <br>
        <h4 style="text-align: center;" >Histogram of Battery Life </h4>
      <img src={% static 'portfolio/clustering/hist.png' %} alt="api-logo" width="500" height="392">
      <br>
      <p>It can be inferred from the above chart that, we can start with having 4 clusters and perform kmeans.</p>
      <br>
      <h4>Finding the optimal K value for Kmeans using Silhoutte Method</h4>
      <br>
      <p>The highest silhoutte score was given for K value 11. So it is the optimal value suggested by this method.</p>
      <img src={% static 'portfolio/clustering/silhoutte.png' %} alt="api-logo" width="500" height="394">
      <br>
  


      <h4>Kmeans - Output</h4>
      <p>The output for the K Value (4), inferred from the histogram above</p>
      <img src={% static 'portfolio/clustering/kmeans_4.png' %} alt="api-logo" width="941" height="263">
      <br>
      <p>On comparing the clusters with the Battery Life category and the discharge capacity plot, it shows that with the input fed we can predict the discharge capacity and the Battery Life category</p>

      
      <p><p>The output for the K Value (11), inferred from the Silhoutte method above</p></p>
      <img src={% static 'portfolio/clustering/Clustering output.png' %} alt="api-logo" width="941" height="263">
      <br>
      <p>This gives more clusters but it offers a indepth insight about some of the clusters than the above result.</p>
      <br>
      <h4>Hierachical Clustering - Output</h4>
      
    
     <br>
      <h4>Dendogram using Complete Linkage</h4>
      <img src={% static 'portfolio/clustering/hclust.png' %} alt="api-logo" width="500" height="500">
      <br>
      <h4>Dendogram using Average Linkage</h4>
    <br>
      <img src={% static 'portfolio/clustering/hclust_average.png' %} alt="api-logo" width="500" height="500">
      <br>

      <h4>Dendogram using Single Linkage</h4>
      <br>

      <img src={% static 'portfolio/clustering/hclust_single.png' %} alt="api-logo" width="500" height="500">
      <br>
 

      <p>Based on the above dendograms, It is inferred to HClust is performed with a K value 6<p>
      <br>
      
      <img src={% static 'portfolio/clustering/hclust_scatter.png' %} alt="api-logo" width="500" height="500">
      <br>
     <p>This is similar to the clusters based on the Kmeans clustering but since the K value is 6 it has split some clusters providing some valid insights for the data discretization process.</p>
      <h3>Conclusion</h3>
      <p>Integrating clustering analysis into our modeling framework not only provided valuable insights into battery performance but also significantly enriched our predictive capabilities for both Remaining Useful Battery Life (RUL) and discharge capacity. By stratifying the data into distinct clusters based on similarities in key features, we were able to develop cluster-specific predictive models tailored to the unique degradation patterns observed within each cluster. This approach not only enhanced the accuracy of our battery life predictions but also enabled more precise estimates of discharge capacity over time. For instance, clusters exhibiting accelerated degradation rates were characterized by distinct sets of predictive variables, allowing us to identify early warning signs of capacity loss and predict battery life more accurately than traditional methods.</p>
      <p>Furthermore, the clustering analysis facilitated the identification of critical features driving variations in both battery life and discharge capacity across different battery states. By isolating the most influential variables within each cluster, we were able to refine our predictive models and capture subtle nuances in battery degradation dynamics more effectively. This comprehensive understanding of the underlying factors influencing battery life and discharge capacity enabled us to develop predictive models that are not only more accurate but also more robust to real-world variability. As a result, our approach holds immense promise for optimizing battery management strategies and prolonging the operational lifespan of battery systems in diverse applications, ranging from electric vehicles to renewable energy storage.</p>





</section>

<section id="section4"> 
    <h2>ARM</h2>
    <img src={% static 'portfolio/clustering/arm_main.png' %} alt="api-logo" width="500" height="500">
    <a href="https://www.analytixlabs.co.in/blog/apriori-algorithm-in-data-mining/" target="_blank">
        Source:https://www.analytixlabs.co.in/blog/apriori-algorithm-in-data-mining/
    </a>
    <br>
    <br>
    <h4>Overview</h4>
   <p>Association Rule Mining (ARM) is a data mining technique used to discover relationships or associations between items or variables in a dataset. It is commonly applied to transactional data, where each transaction consists of a set of items purchased together. ARM aims to uncover patterns in these transactions that can provide valuable insights for various applications, such as market basket analysis and recommendation systems.

    For example, consider a retail store that records transactions from its customers. Each transaction contains a list of items purchased by a customer during a single visit to the store. By applying ARM to this transactional data, the store can discover associations between different items frequently purchased together. These associations can be represented as rules of the form "If {A} then {B}", indicating that customers who buy item A are likely to buy item B as well.</p> 
    <h4>Description</h4>
<br>
    <img src={% static 'portfolio/clustering/arm_formula.png' %} alt="api-logo" width="500" height="500">
    <a href="https://bicorner.com/2015/07/22/what-the-heck-are-association-rules-in-analytics/" target="_blank">
        Source:https://bicorner.com/2015/07/22/what-the-heck-are-association-rules-in-analytics/
    </a>
    <br><br>
    <p>ARM involves analyzing transactional data to find relationships or associations between items or variables in the data. It helps in various areas such as market basket analysis and recommendation systems.</p>
    
    <h4>Support</h4>
    <p>Support is a measure used in ARM to indicate the frequency of occurrence of a particular itemset in the dataset. It helps identify the most frequent itemsets or combinations of items.</p>
    
    <h4>Confidence</h4>
    <p>Confidence is a measure used in ARM to indicate the reliability of a rule. It measures the likelihood that an item B is purchased given that item A is purchased.</p>
    
    <h4>Lift</h4>
    <p>Lift is a measure used to evaluate the strength of association between two items. It compares the likelihood of two items occurring together to the likelihood of their occurrence being independent of each other.</p>
    
    <h4>What are rules?</h4>
    <p>Association rules are statements that describe the relationships between items in a dataset. They are typically in the form of "If {A} then {B}", where A and B are itemsets.</p>
    
    <h4>What is Apriori?</h4>
    <p>The Apriori algorithm is a popular algorithm used for mining association rules. It works by iteratively generating candidate itemsets and pruning those that do not meet the minimum support threshold.</p>
    
    <h4>Its Working</h4>
    <p>The Apriori algorithm starts by identifying frequent itemsets through iterative candidate generation and pruning. It then generates association rules based on these frequent itemsets, providing valuable insights into the data.</p>

    <h3>Data Prep </h3>
    <h4>Conversion of Data into Transaction Data</h4>
    <p>Transaction data is typically represented as a dataset where each row corresponds to a transaction, and the items purchased in that transaction are listed. To convert raw data into transaction data suitable for Association Rule Mining (ARM), you need to aggregate the data into transactional format. This is commonly done by identifying unique transactions and listing the items associated with each transaction.</p>
    <p>For example, consider a dataset containing sales data for a retail store. Each row in the dataset represents a sale, with columns indicating the transaction ID and the items purchased. To convert this data into transaction format, you would aggregate the items purchased in each transaction and create a list of items associated with each unique transaction ID.</p>
    
    <h4>Why Unnlabelled Transaction Data is used?</h4>
    <p>Unlabeled transaction data is preferred in Association Rule Mining (ARM) due to its flexibility and ability to uncover hidden patterns without predefined labels or categories. With unlabeled data, there's no need to specify predefined categories for items, allowing for the discovery of associations between any items present in the dataset. This approach enables the extraction of insights from raw transactional data, particularly in real-world scenarios where items may not come with predefined labels. Analyzing unlabeled transaction data simplifies the analysis process and facilitates the discovery of generalizable patterns that hold across different contexts. By focusing on the co-occurrence of items in transactions, ARM algorithms can identify common purchasing behaviors and item relationships, providing valuable insights for various industries and domains.</p>
   
    <div>
        <span class="indented">
    <h4>Code : <a href="https://github.com/santhosh1299/Project/blob/main/API_news.ipynb">News data collection from API</a></h4>
</span>
</div>
<br>
    <h4>Before Preprocessing</h4>
    <br>
    <p>Json Response of the news data using API</p>
    <img src={% static 'portfolio/clustering/arm_before.png' %} alt="api-logo" width="941" height="263">
    <br>
    <h4>After Preprocessing</h4>
    <br>
    <p>It contains the itemsets and the transaction ID for each transaction</p>
    <img src={% static 'portfolio/clustering/arm_after.png' %} alt="api-logo" width="500" height="463">
    <br>
  
    <h3>Code </h3>


<div>
    <span class="indented">
    <h4>Code : <a href="https://github.com/santhosh1299/Project/blob/main/Clustering-kmeans.ipynb">Association Rule Mining in R</a></h4>
</span>
</div>
<br><br>
   <h3>Results</h3>

   <br>
   <p>Association Network visualisation</p>
   <img src={% static 'portfolio/clustering/network.png' %} alt="api-logo" width="500" height="500">
   <br>
   <p>
    From the ARM network analysis of recent news text data, it becomes apparent that certain keywords such as "pixel," "meta," "quest," "hour," and others were prominently featured. Upon closer examination of this data, it is evident that there has been a significant focus on topics related to the Meta Quest Pro and the Apple iPhone over the past month. This suggests a notable trend in the news landscape, with increased coverage and discussions revolving around these particular subjects. Such insights gleaned from the analysis underscore the relevance and impact of these technological developments within the media sphere, reflecting evolving consumer interests and industry dynamics.</p>
   <br>
   <h4>Top 10 rules by Support</h4>
   <br>

   <img src={% static 'portfolio/clustering/arm_support.png' %} alt="api-logo" width="600" height="371">
   <br>
   <p>An intriguing observation from these association rules is the strong presence of specific electronic devices like "iphone" and "galaxy" in conjunction with mentions of "battery". The rule ({iphone} => {battery}) stands out with a relatively high confidence of 61.54% and a lift value of 8.58, indicating a significant association between discussions related to iPhones and the presence of battery-related topics. This suggests that battery performance is a crucial consideration in conversations or activities involving iPhones, possibly reflecting the importance of battery life in mobile devices. Similarly, the association between "galaxy" and "battery" is notable, although with a lower confidence and lift value compared to iPhones. Nonetheless, this association implies a correlation between discussions surrounding Galaxy devices and battery-related concerns, highlighting the significance of battery performance across different smartphone brands. Additionally, the presence of terms like "power" and "portable" further emphasizes the relevance of battery-related discussions in various contexts, from portable electronics to electric vehicles. Overall, these association rules shed light on the interconnectedness between electronic devices and battery-related topics, offering valuable insights into consumer preferences and technological considerations.</p>
   <br>
   <h4>Top 10 rules by Confidence</h4>
   <br>

   <img src={% static 'portfolio/clustering/arm_confidence.png' %} alt="api-logo" width="600" height="371">
   <br>
   <p>One interesting aspect revealed by these association rules is the strong correlation between certain items or features and the presence of "battery" in the dataset. For instance, the rule ({iphone, new} => {battery}) stands out with a perfect confidence score, suggesting that whenever discussions or activities involving both "iphone" and "new" arise, the mention of "battery" is almost guaranteed. This association highlights a potential trend where the introduction of new iPhone models coincides with discussions about battery-related topics, indicating a significant focus on battery performance and technology advancements within the smartphone industry. Additionally, the association between "speed" and "battery" ({speed} => {battery}) showcases an interesting relationship, with a relatively high confidence score indicating a strong likelihood of battery mentions in contexts related to speed. This could hint at discussions surrounding the battery performance of high-speed electronic devices or electric vehicles, where power demands are often paramount. Overall, these association rules offer valuable insights into the interconnectedness of different features within the dataset, shedding light on underlying patterns and potential relationships that merit further exploration and analysis.</p>
   <h4>Top 10 rules by Lift</h4>
   <br>

   <img src={% static 'portfolio/clustering/arm_lift.png' %} alt="api-logo" width="600" height="371">
   <br>
  <p>The notable correlation between specific items or features and the mention of "battery" within the dataset. For instance, the rule ({iphone, new} => {battery}) stands out with a remarkably high lift value of 13.94, indicating a significant association between discussions or activities involving both "iphone" and "new" and the presence of "battery" in the same context. This suggests a compelling trend where the introduction of new iPhone models coincides with discussions about battery-related topics, underscoring the importance of battery performance in the realm of smartphone technology. Additionally, the rule ({speed} => {battery}) is particularly interesting due to its high lift value of 9.29, suggesting a strong correlation between discussions related to "speed" and mentions of "battery". This could imply a focus on battery performance in high-speed electronic devices or electric vehicles, where power demands are crucial. Overall, these association rules provide valuable insights into the interconnectedness of different features within the dataset, offering intriguing patterns and relationships that warrant further exploration and analysis.</p>
    <br>
<br>
<h3> Some Interesting Association with frequently occurring keywords</h3>
<br><br>
   <h4>The association based on "IPhone" keyword on the LHS</h4>
   <img src={% static 'portfolio/clustering/iphone_network.png' %} alt="api-logo" width="500" height="500">
   <br>

   <br>
   <h4>The association based on "Galaxy" keyword on the LHS</h4>
   <img src={% static 'portfolio/clustering/galaxy_network.png' %} alt="api-logo" width="500" height="500">
   <br>

   <br>
   <h4>The association based on "EV" keyword on the LHS</h4>
   <img src={% static 'portfolio/clustering/ev_network.png' %} alt="api-logo" width="500" height="500">
   <br>


<p>One particularly interesting association rule within the dataset is {iphone} => {battery}. With a relatively high support of 1.85%, this rule indicates that the purchase or discussion of iPhones often coincides with mentions of batteries. The confidence value of 61.54% suggests a strong predictive power, implying that when iPhones are present, there's a notable likelihood (approximately 61.54%) that batteries are also mentioned. Additionally, the lift value of 2.21 indicates a positive correlation between iPhones and batteries, meaning that the presence of iPhones tends to increase the likelihood of batteries being mentioned beyond what would be expected by chance alone. This association may reflect the importance of battery life in mobile devices like iPhones, influencing consumer discussions and purchases. Overall, this rule highlights a significant and meaningful association between iPhones and batteries, offering insights into current news related to batteries.</p>
<br><br>
<h3>Conclusion</h3>
<br>
<p>In summary, when we looked at news stories from the last month, we found some interesting connections between different topics, especially in the world of technology. One big discovery was how often people talk about iPhones and batteries together. It seems like whenever iPhones are in the news, batteries aren't far behind. We also noticed that other topics like Meta Quest, electric cars, and market trends are all linked to discussions about batteries. This shows us what's catching people's attention and driving conversations in the news.</p>
<p>Moreover, lately, there's been a lot of talk about battery problems not only with iPhones but also with Meta Quest and electric cars. This tells us that battery issues are becoming more important across different parts of the tech industry. With more focus on sustainability and saving energy, understanding these trends is crucial for businesses to keep up with what consumers want.</p>
<p>In short, by looking at how different topics connect in the news, we can learn a lot about what's happening in the world. This information helps businesses adjust their plans to match what people care about and where technology is heading. Whether it's fixing battery problems or following new market trends, paying attention to these connections in the news helps companies stay on top of what's important to their customers.</p>
</section>

<section id="section5">
    <h2>NaiveBayes</h2>
    <img src={% static 'portfolio/NB & DecTree/nb_1.jpg' %} alt="api-logo" width="500" height="500">
    <br>
    <h3>Overview</h3>
    <p>Naive Bayes is a simple but powerful probabilistic classification algorithm based on Bayes' theorem with the "naive" assumption of independence between features. Despite its simplicity, it's widely used in various applications due to its efficiency and effectiveness, particularly in text classification and spam filtering.</p>

<p>One of the key applications of Naive Bayes is in text classification tasks. It's commonly used for sentiment analysis, document categorization, topic classification, and spam detection. In sentiment analysis, Naive Bayes can determine whether a piece of text expresses positive, negative, or neutral sentiment based on the words used. For document categorization, it can classify documents into predefined categories such as news articles, emails, or research papers. In spam detection, Naive Bayes can identify whether an email is spam or not based on its content and other features.</p>

<p>Another important application of Naive Bayes is in medical diagnosis and healthcare analytics. It can be used to predict the likelihood of a patient having a particular disease based on symptoms, test results, and other medical data. Naive Bayes classifiers have been employed in various medical domains, including cancer detection, disease risk assessment, and patient outcome prediction.</p>

<h2>Applications of Naive Bayes</h2>

<p>Naive Bayes is a versatile classification algorithm with numerous applications across different domains:</p>

<ul>
  <li><strong>Text Classification:</strong> Naive Bayes is extensively used in sentiment analysis, document categorization, topic classification, and spam detection tasks. It can analyze text data to determine sentiment, categorize documents, identify topics, and detect spam emails efficiently.</li>

  <li><strong>Healthcare Analytics:</strong> In medical diagnosis, Naive Bayes can predict the likelihood of a patient having a particular disease based on symptoms and medical data. It's applied in cancer detection, disease risk assessment, and patient outcome prediction.</li>

  <li><strong>Recommendation Systems:</strong> Naive Bayes is employed in recommendation systems to analyze user preferences and item features, recommending products, movies, music, or articles based on user interests.</li>

  <li><strong>Fraud Detection:</strong> In fraud detection, Naive Bayes identifies potentially fraudulent transactions or activities by analyzing patterns and features associated with known fraudulent behavior.</li>

  <li><strong>Anomaly Detection:</strong> Naive Bayes detects unusual patterns in data, such as network intrusions or credit card fraud, by identifying deviations from normal behavior.</li>

  <li><strong>Document Summarization:</strong> Naive Bayes aids in summarizing documents by extracting key information and identifying important content.</li>

  <li><strong>Customer Segmentation:</strong> It helps in segmenting customers based on their behavior, preferences, and demographics, facilitating targeted marketing strategies.</li>

  <li><strong>Market Basket Analysis:</strong> Naive Bayes is used to analyze customer purchase patterns and identify associations between items bought together in transactions.</li>

  <li><strong>Weather Prediction:</strong> In meteorology, Naive Bayes assists in predicting weather conditions by analyzing historical data and environmental factors.</li>
</ul>


<h2>Standard Multinomial Naive Bayes</h2>

<p>Standard Multinomial Naive Bayes is a variant of the Naive Bayes algorithm that is specifically designed for text classification tasks, where the features are typically discrete word counts or frequencies. It's widely used in natural language processing (NLP) applications such as sentiment analysis, document categorization, and spam filtering. Let's delve into the details of how Multinomial Naive Bayes works and its key components:</p>

<ul>
  <li><strong>Probability Model:</strong> Multinomial Naive Bayes models the probability distribution of word occurrences in documents using a multinomial distribution. It assumes that each feature (word) follows a multinomial distribution conditioned on the class label.</li>

  <li><strong>Feature Representation:</strong> In text classification tasks, documents are represented as feature vectors, where each feature represents the count or frequency of a word in the document. Typically, a bag-of-words or TF-IDF (Term Frequency-Inverse Document Frequency) representation is used.</li>

  <li><strong>Class Prior Probability:</strong> Multinomial Naive Bayes calculates the prior probability of each class based on the frequency of class occurrences in the training data. This provides the initial probability of each class before observing any features.</li>

  <li><strong>Conditional Probability:</strong> The algorithm computes the conditional probability of observing each feature (word) given the class label using the multinomial distribution. It estimates the likelihood of observing a particular word in documents belonging to a specific class.</li>

  <li><strong>Naive Assumption:</strong> Similar to the standard Naive Bayes algorithm, Multinomial Naive Bayes makes the "naive" assumption of feature independence, i.e., it assumes that the occurrence of each word is independent of the occurrences of other words within the same document, given the class label.</li>

  <li><strong>Laplace Smoothing:</strong> To handle the issue of zero probabilities for unseen words in the test data, Laplace smoothing (or additive smoothing) is often applied. It adds a small smoothing parameter to the word counts to ensure that no probability becomes zero.</li>

  <li><strong>Classification Decision:</strong> Finally, to classify a new document, Multinomial Naive Bayes applies Bayes' theorem to compute the posterior probability of each class given the observed features. The class with the highest posterior probability is assigned as the predicted class for the document.</li>
</ul>

<p><strong>Benefits of Multinomial Naive Bayes:</strong></p>
<ul>
  <li>Efficient: It's computationally efficient and scales well with large datasets.</li>
  <li>Effective for Text Data: Well-suited for text classification tasks due to its ability to handle discrete features like word counts.</li>
  <li>Simple and Interpretable: The algorithm is easy to understand and interpret, making it suitable for both beginners and experts in NLP.</li>
</ul>



<h2>Bernoulli Naive Bayes</h2>

<p>Bernoulli Naive Bayes is another variant of the Naive Bayes algorithm commonly used in text classification tasks, particularly when the feature vectors are binary-valued. It's well-suited for applications such as sentiment analysis, document categorization, and spam filtering. Let's explore the details of how Bernoulli Naive Bayes operates and its key aspects:</p>

<ul>
  <li><strong>Probability Model:</strong> Bernoulli Naive Bayes models the probability distribution of binary feature occurrences in documents using a Bernoulli distribution. It assumes that each feature (word) follows a Bernoulli distribution conditioned on the class label.</li>

  <li><strong>Feature Representation:</strong> In Bernoulli Naive Bayes, documents are represented as binary feature vectors, where each feature indicates the presence (1) or absence (0) of a word in the document. This binary representation is suitable for scenarios where only the presence of words matters, not their frequencies.</li>

  <li><strong>Class Prior Probability:</strong> Similar to other Naive Bayes variants, Bernoulli Naive Bayes calculates the prior probability of each class based on the frequency of class occurrences in the training data, providing the initial probability of each class before observing any features.</li>

  <li><strong>Conditional Probability:</strong> The algorithm computes the conditional probability of observing each feature (word) given the class label using the Bernoulli distribution. It estimates the likelihood of observing a particular word in documents belonging to a specific class.</li>

  <li><strong>Naive Assumption:</strong> Bernoulli Naive Bayes, like other Naive Bayes algorithms, makes the "naive" assumption of feature independence, assuming that the presence or absence of each word is independent of other words within the same document, given the class label.</li>

  <li><strong>Laplace Smoothing:</strong> To handle zero probabilities for unseen words in the test data, Laplace smoothing (or additive smoothing) can be applied in Bernoulli Naive Bayes. It adds a small smoothing parameter to the feature counts to prevent zero probabilities.</li>

  <li><strong>Classification Decision:</strong> To classify a new document, Bernoulli Naive Bayes applies Bayes' theorem to compute the posterior probability of each class given the observed features. The class with the highest posterior probability is assigned as the predicted class for the document.</li>
</ul>

<p><strong>Benefits of Bernoulli Naive Bayes:</strong></p>
<ul>
  <li>Effective for Binary Features: Well-suited for text classification tasks with binary feature representations, such as presence or absence of words.</li>
  <li>Simple and Efficient: Like other Naive Bayes algorithms, Bernoulli Naive Bayes is computationally efficient and easy to implement.</li>
  <li>Interpretability: The algorithm provides interpretable results, making it suitable for explaining the classification decisions.</li>
</ul>


<h2>Data Prep</h2>

<h3>Before Cleaning</h3>

<h4>Supervised Learning models will take only labelled data and python only take numerical input.</h4>
   <img src={% static 'portfolio/NB & DecTree/nb_before.png' %} alt="api-logo" width="500" height="500">
   <br>

<h3>After Cleaning</h3>

<h4>All the categorical columns are converted to numerical values.</h4>
   <img src={% static 'portfolio/NB & DecTree/nb_after.png' %} alt="api-logo" width="500" height="500">
   <br>

   
<p>We divided the dataset into two parts: one for training the model and one for testing its accuracy. We used an 80-20 split, meaning 80% of the data was used for training and 20% for testing. This separation helps ensure the model performs well on new, unseen data. By testing on data the model hasn't seen before, we can see how well it works in different situations. This process helps us make sure the model gives reliable results and works effectively in real-world situations.</p>
<br>   
<h3>Code </h3>


   <div>
       <span class="indented">
       <h4>Code : <a href="https://github.com/santhosh1299/lithium_ion_battery_discharge_prediction/blob/main/naive_bayes_and_decision_tree.ipynb">Naive Bayes</a></h4>
   </span>
   <h3>Results</h3>

   <br>
   <p>We noticed that our data had a significant imbalance between classes, meaning some classes were much more prevalent than others. Despite achieving 63% accuracy with a multinomial Naive Bayes model, upon analyzing the confusion matrix, we found discrepancies that raised concerns. To address this, we employed SMOTE, a technique for mitigating class imbalance by generating synthetic samples. However, after implementing SMOTE and retraining the multinomial model, we were disappointed to find a drastic drop in accuracy to only 28%. This outcome puzzled us, leading us to reconsider our approach. We realized that multinomial Naive Bayes is typically suited for text data or categorical data with count frequencies, whereas our data consisted of numerical features. Therefore, we decided to pivot to a Gaussian Naive Bayes model, which is better suited for numerical data. This adjustment was made with the hope of improving our model's performance by aligning it more closely with the nature of our dataset.</p>
   <h4>Multinomial Naive Bayes model (Accuracy 70%) with imbalanced data</h4>
   <img src={% static 'portfolio/NB & DecTree/result_nb_1.png' %} alt="api-logo" width="500" height="500">
   <br>
   <p>After applying the SMOTE technique to the training data</p>
   <br>
   <h4>Multinomial Naive Bayes model (Accuracy 28%) with balanced data</h4>
   <br>

   <img src={% static 'portfolio/NB & DecTree/result_nb_2.png' %} alt="api-logo" width="600" height="371">
   <br>
   <p>The model performed poorly because Standard Multinomial Naive Bayes is suitable for text data or categorical data with count frequencies. So, we have trained the data with Gaussian Naive Bayes model as it takes numerical values  </p>
   <br>
   <h4>Gaussian Naive Bayes model (Accuracy 95%) with balanced data</h4>
   <br>

   <img src={% static 'portfolio/NB & DecTree/result_nb_3.png' %} alt="api-logo" width="600" height="371">
   <br>
   <h3>Conclusion</h3>
   <br>
   <p>The results of predicting how long batteries will last are really important for different industries. When we know how long a battery will work, we can plan ahead and fix things before they break. This is especially useful for things like planes, cars, and renewable energy sources that rely on batteries. It helps us avoid accidents and saves time and money by fixing things before they break.</p>

<p>But it's not just about fixing things. Knowing how long batteries last helps us use them better. For example, in electric cars, we can make sure batteries are used efficiently and last longer. It also helps us use renewable energy sources, like solar and wind power, more reliably. And in healthcare, it ensures medical devices work when they're needed most.</p>

<p>Overall, predicting battery life helps us work smarter, save money, and keep things running smoothly. By using fancy computer programs and new technology, we can make the world a better place and make sure things keep working for a long time.</p>




</section>

<section id="section6">
    <h2>DecTrees</h2>
    <img src={% static 'portfolio/NB & DecTree/dtree_1.jpg' %} alt="api-logo" width="500" height="500">
    <br>
    <h3>Overview</h3>
    <p>Decision Trees are like maps that guide decisions based on a series of questions. Each question leads to more questions until a conclusion is reached. Imagine navigating a flowchart where each step helps determine the best course of action. Similarly, Decision Trees start with a broad question and gradually narrow down options based on the answers provided. This intuitive approach makes Decision Trees easy to understand, even for those unfamiliar with technical concepts.</p>
    <p>Now, let's delve into how Decision Trees are trained. During training, the algorithm learns from existing data to create this decision-making map. It analyzes different features, such as age or income, and splits the data into groups based on the most informative questions. By repeatedly asking and refining these questions, the algorithm constructs a tree-like structure that accurately predicts outcomes. This process mimics how humans make decisions, making it accessible and relatable to non-technical team members.</p>
    <p>As the Decision Tree is trained, it aims to minimize confusion and maximize clarity in decision-making. It's like organizing information into categories, making it easier to process and act upon. Through this iterative process, the Decision Tree becomes a valuable tool for guiding future decisions, empowering teams to navigate complex scenarios with confidence.</p>
    <img src={% static 'portfolio/NB & DecTree/dtree_2.png' %} alt="api-logo" width="500" height="500">
    <br>
    <h2>Decision Making Process</h2>
    <ul>
      <li><strong>Tree Structure:</strong> Decision Trees start with a root node representing the entire dataset and recursively split the data into smaller subsets as they move down the tree.</li>
      <li><strong>Node Splitting:</strong> At each node, the algorithm selects features and split points to maximize subset homogeneity.</li>
      <li><strong>Stopping Criteria:</strong> The splitting process continues until specific criteria, such as maximum depth or minimum impurity, are met.</li>
    </ul>
  
    <h2>Training Process</h2>
    <p><strong>Overview:</strong> During training, Decision Trees learn from existing data to create decision-making maps.</p>
    <h3>Feature Selection</h3>
    <ul>
      <li><strong>Criteria:</strong> Features are evaluated based on criteria such as Gini impurity or entropy to determine the best splits.</li>
      <li><strong>Information Gain:</strong> Information gain quantifies the reduction in impurity achieved by splitting data on a particular feature.</li>
    </ul>
    <h3>Recursive Splitting</h3>
    <ul>
      <li><strong>Partitioning Data:</strong> The algorithm partitions the dataset into subsets based on feature values to create homogeneous subsets.</li>
      <li><strong>Stopping Criteria:</strong> The recursive splitting process continues until stopping criteria, like maximum depth or minimum impurity decrease, are met.</li>
    </ul>
  
    <h2>Prediction Process</h2>
    <p><strong>Overview:</strong> Predictions for new instances involve traversing the tree from the root to a leaf node based on feature values.</p>
    <h3>Traversal</h3>
    <ul>
      <li><strong>Decision Path:</strong> The tree is traversed, evaluating feature values at each node to determine the appropriate branch.</li>
      <li><strong>Leaf Node Prediction:</strong> Once a leaf node is reached, the associated prediction is returned.</li>
    </ul>

    <h2>Data Prep</h2>

    <h3>Before Cleaning</h3>
    
    <h4>Supervised Learning models will take only labelled data and python only take numerical input.</h4>
       <img src={% static 'portfolio/NB & DecTree/nb_before.png'' %} alt="api-logo" width="700" height="700">
       <br>
    
    <h3>After Cleaning</h3>
    
    <h4>All the categorical columns are converted to numerical values.</h4>
       <img src={% static 'portfolio/NB & DecTree/nb_after.png'' %} alt="api-logo" width="700" height="700">
       <br>

<br>       
<p>We divided the dataset into two parts: one for training the model and one for testing its accuracy. We used an 80-20 split, meaning 80% of the data was used for training and 20% for testing. This separation helps ensure the model performs well on new, unseen data. By testing on data the model hasn't seen before, we can see how well it works in different situations. This process helps us make sure the model gives reliable results and works effectively in real-world situations.</p>
       <div>
        <span class="indented">
        <h4>Code : <a href="https://github.com/santhosh1299/lithium_ion_battery_discharge_prediction/blob/main/naive_bayes_and_decision_tree.ipynb">Decision Tree</a></h4>
    </span>
    <h3>Results</h3>
 
     <h4>Decision Tree with Gini as the criteria and other default parameters (Accuracy 98%)</h4>
    <img src={% static 'portfolio/NB & DecTree/result_dt_1.png' %} alt="api-logo" width="500" height="500">
    <br>
    <img src={% static 'portfolio/NB & DecTree/tree_1.png' %} alt="api-logo" width="500" height="500">
    <p>The tree structure appears overly intricate, featuring an abundance of branches.</p>
    <br>
    <h4>Decision Tree with entropy as the criteria and max_depth as 3 (Accuracy 96%)</h4>
    <img src={% static 'portfolio/NB & DecTree/result_dt_2.png' %} alt="api-logo" width="500" height="500">
    <br>
    <img src={% static 'portfolio/NB & DecTree/tree_2.png' %} alt="api-logo" width="500" height="500">
    <p>The tree complexity is reduced</p>
    <br>
    <h4>Decision Tree with entropy as the criteria, max_depth as 3 and max_features as 3 (Accuracy 93%)</h4>
    <img src={% static 'portfolio/NB & DecTree/result_dt_3.png' %} alt="api-logo" width="500" height="500">
    <br>
    <img src={% static 'portfolio/NB & DecTree/tree_3.png' %} alt="api-logo" width="500" height="500">
    <p>The tree complexity is reduced and the tree looks simple so this will give an generalized and optimal prediction.</p>
    <br>
 
    <h3>Conclusion</h3>
    <br>
    <p>The results of predicting battery life are really important for different industries. When we know the life expectancy of a battery, we can plan ahead and fix things before they break. This is especially useful for things like planes, cars, and renewable energy sources that rely on batteries. It helps us avoid accidents and saves time and money by fixing things before they break.</p>
    
    <p>But it's not just about fixing things. Knowing battery life helps us use them better. For example, in electric cars, we can make sure batteries are used efficiently and last longer. It also helps us use renewable energy sources, like solar and wind power, more reliably. And in healthcare, it ensures medical devices work when they're needed most.</p>
    
    <p>Overall, predicting battery life helps us work smarter, save money, and keep things running smoothly. By using fancy computer programs and new technology, we can make the world a better place and make sure things keep working for a long time.</p>
    

</section>

<section id="section7">
    <h2>SVM</h2>
    <img src={% static 'portfolio/NB & DecTree/nb_1.jpg' %} alt="api-logo" width="500" height="500">
    <br>
    <h3>Overview</h3>
    <p>Support Vector Machine (SVM) is a powerful supervised machine learning algorithm that is widely used for classification and regression tasks, it is mainly used in image processing.</p> 
    <p>SVM aims to find the optimal hyperplane that best separates data points belonging to different classes by maximizing the margin between them.This not allows the SVM to classify the data but also this prevents the model from overfitting, to the high variance of the data.</p>
    <br>
    <h3>How SVM Works</h3>
    <p>SVM works by finding the optimal hyperplane that best separates the data points belonging to different classes. The hyperplane is chosen such that it maximizes the margin between the nearest data points of different classes, known as support vectors. This margin represents the distance between the hyperplane and the support vectors and serves as a measure of the model's generalization capability. This model acheives this by optimzation methods like gradient descent.</p>
    
    <h3>Kernel Functions</h3>
    <br>
    <img src={% static 'portfolio/svm/kernel.png' %} alt="api-logo" width="500" height="500">
    <br>
    <p>Kernel functions play a crucial role in SVM, especially when dealing with non-linearly separable data. These functions transform the input data into a higher-dimensional space where it becomes linearly separable. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid. By employing appropriate kernel functions, SVM can effectively handle complex data distributions and improve its classification performance.</p>
    
    <h3>Optimization</h3>
    <p>Optimization lies at the heart of SVM, as it involves finding the parameters of the hyperplane that minimize a predefined loss function. This optimization problem can be formulated as a convex optimization task, which allows for efficient and reliable solutions. SVM employs optimization techniques such as gradient descent and quadratic programming to iteratively adjust the parameters until convergence is achieved.</p>
    
    <h3>Margin Maximization</h3>
    <br>
    <img src={% static 'portfolio/svm/margin.png' %} alt="api-logo" width="500" height="500">
    <br>
    <p>One of the key principles of SVM is margin maximization, which aims to maximize the distance between the decision boundary (hyperplane) and the nearest data points of different classes. By maximizing the margin, SVM not only improves its classification accuracy on the training data but also enhances its ability to generalize to unseen data. This margin-based approach makes SVM robust against outliers and noise in the data.</p>
    

    <h3>Why They Are Linear Separators</h3>
    <p>Support Vector Machines (SVMs) are considered linear separators because they aim to find the optimal hyperplane that best separates data points belonging to different classes. This hyperplane is a linear decision boundary in the input space, represented as a line in two dimensions, a plane in three dimensions, and a hyperplane in higher dimensions. SVMs achieve classification by maximizing the margin between this hyperplane and the nearest data points of different classes, known as support vectors. This margin-based approach allows SVMs to effectively classify data points by constructing a linear decision boundary, making them suitable for linearly separable datasets.</p>

    <h3>How the Kernel Works</h3>
    <p>The kernel function in Support Vector Machines (SVMs) is a mathematical function that computes the dot product between two points in a higher-dimensional space. It allows SVMs to implicitly map the input space into a higher-dimensional feature space without explicitly calculating the transformation.</p>
    

    <h3>Radial Basis Function (RBF) Kernel:</h3>
    <p>The Radial Basis Function kernel is defined as:</p>
    <p>K(x, y) = exp(-γ ||x - y||^2)</p>
    <p>Where:</p>
    <ul>
        <li>x and y are input vectors</li>
        <li>γ is a parameter that determines the influence of each training example</li>
        <li>||x - y|| represents the Euclidean distance between vectors x and y</li>
    </ul>

    <h3>Polynomial Kernel:</h3>
    <p>The Polynomial kernel is defined as:</p>
    <p>K(x, y) = (x · y + c)^d</p>
    <p>Where:</p>
    <ul>
        <li>x and y are input vectors</li>
        <li>c is a constant</li>
        <li>d is the degree of the polynomial</li>
    </ul>
    <br>
    <h4>Example of Casting a 2D point and casting into 6D using Polynomial Kernel.</h4>
<br>    

<img src={% static 'portfolio/svm/kernel_1.png' %} alt="api-logo" width="500" height="500">

<img src={% static 'portfolio/svm/kernel_2.png' %} alt="api-logo" width="500" height="700">









    <h3>Importance of Dot Product</h3>
    <p>The dot product is critical because it captures the similarity or dissimilarity between two points in the input space. By computing the dot product in the higher-dimensional space, the kernel function effectively measures the relationship between data points, enabling SVMs to find complex decision boundaries.</p>


    <h3>Applications</h3>
    <p>SVM finds applications across a wide range of domains due to its versatility and robustness. Some common applications of SVM include:</p>
    <ul>
        <li>Image classification</li>
        <li>Text categorization</li>
        <li>Handwritten digit recognition</li>
        <li>Bioinformatics</li>
        <li>Medical diagnosis</li>
        <li>Financial forecasting</li>
    </ul>
    <p>Due to its ability to handle imbalanced data, SVM was the best performing model for this battery life prediction compared to the other models that was trained on the same data.</p>
    
    <h3>Advantages of SVM</h3>

    <ul>
        <li>Effective in high-dimensional spaces</li>
        <li>Robust against overfitting- robust to outliers</li>
        <li>Works well with both linear and non-linear data</li>
        <li>Ability to capture complex classifications</li>
        <li>Works well with small datasets</li>
    </ul>
    <h3>Disadvantages of SVM</h3>
    <ul>
        <li>Can be computationally intensive, especially with large datasets</li>
        <li>Requires careful selection of kernel function and parameters</li>
        <li>Does not provide probabilistic outputs directly</li>
        <li>May struggle with noisy or overlapping data</li>
        <li>Interpretability of the model may be limited</li>
    </ul>
    <p>Overall, while SVM is a powerful and versatile algorithm. I have an experience of where SVM took around 2 hours to train on a large dataset.</p>

    <h2>Data Prep</h2>

    <h3>Before Cleaning</h3>
    
    <p>Supervised Learning models will take only labelled data and python only take numerical input.</p>
       <img src={% static 'portfolio/svm/data_before.png'' %} alt="api-logo" width="700" height="700">
       <br>
    
    <h3>After Cleaning</h3>
    
    <p>All the categorical columns are converted to numerical values.</p>
       <img src={% static 'portfolio/svm/data_after.png'' %} alt="api-logo" width="700" height="700">
       <br>

       <h3>Code </h3>


       <div>
           <span class="indented">
           <h4>Code : <a href="https://github.com/santhosh1299/lithium_ion_battery_discharge_prediction/blob/main/svm.ipynb">Support Vector Machine</a></h4>
       </span>
<br><br>
       <h3>Results</h3>
 <br>
    <h4>Linear Kernel -  Accuracy 96.9%</h4>
      <img src={% static 'portfolio/svm/linear.png' %} alt="api-logo" width="500" height="500">
      <br>
      <br>
      <h4>Polynomial Kernel (C=1)  -  Accuracy 97.8%</h4>
      <img src={% static 'portfolio/svm/poly_1.png' %} alt="api-logo" width="500" height="500">
      <br>
      <h4>Polynomial Kernel (C=0.1)  -  Accuracy 97.4% </h4>
      <img src={% static 'portfolio/svm/poly_0.1.png' %} alt="api-logo" width="500" height="500">
      <br>
      <br>
      <h4>Polynomial Kernel (C=10)  -  Accuracy 98.01%</h4>
      <img src={% static 'portfolio/svm/poly_10.png' %} alt="api-logo" width="500" height="500">
      <br>
      <br>
      <h4>RBF Kernel (C=1)  -  Accuracy 97.1%</h4>
      <img src={% static 'portfolio/svm/rbf_1.png' %} alt="api-logo" width="500" height="500">
      <br>
      <br>
      <h4>RBF Kernel (C=0.1)  -  Accuracy 95.8% </h4>
      <img src={% static 'portfolio/svm/rbf_0.1.png' %} alt="api-logo" width="500" height="500">
      <br>
      <br>
      <h4>RBF Kernel (C=10)  -  Accuracy 95.8% </h4>
      <img src={% static 'portfolio/svm/rbf_10.png' %} alt="api-logo" width="500" height="500">
      <br>
      <br>
      <h3>Visualisation of Best model with Polynomial Kernel C=10</h3>
      <img src={% static 'portfolio/svm/output_1.png' %} alt="api-logo" width="500" height="500">
      <br>

  <p>The results of our analysis demonstrate the ability of Support Vector Machine (SVM) algorithms in predicting battery life categories within an imbalanced dataset:</p>

  <p>1. SVM outperformed Decision Trees significantly, even without employing techniques like Synthetic Minority Over-sampling Technique (SMOTE) to address class imbalance.</p>
  
  <p>2. Different kernels and regularization parameters had varying impacts on SVM's performance:
    <ul>
      <li>Polynomial kernels, especially with ( C = 10 ), achieved exceptional accuracy, capturing complex nonlinear relationships inherent in battery life prediction.</li>
      <li>The radial basis function (RBF) kernel, while slightly less accurate, demonstrated the ability to handle nonlinearity in the data.</li>
      <li>Fine-tuning the regularization parameter ( C ) proved crucial, with lower values resulting in decreased accuracy, emphasizing the importance of controlling model complexity.</li>
    </ul>
      <br>
      <p>In conclusion, SVM emerges as a great option for predicting battery life categories, particularly when dealing with imbalanced datasets. Its nature to handle various types of data and the ability to fine-tune hyperparameters for better performance, positions SVM as a valuable tool in addressing real-world challenges in predictive modeling. These findings highlights the potential of SVM algorithms to drive insights and advancements in various domains reliant on accurate predictive analytics.</p>
      <p>Further research could explore the integration of SVM with other advanced techniques, such as ensemble learning or deep learning, to enhance predictive performance and address more complex predictive tasks in the domain of battery life prediction.</p>

</section>



<section id="section10">
    <h2>Conclusion</h2>

<p>⁤Looking ahead, lithium-ion batteries are becoming important in a wide range of application areas. ⁤⁤While they've long been used in devices like phones and laptops small portable electronics but they're now also essential in sectors such as sustain energy, automobile and power grid storage systems. ⁤⁤The Tesla movement, which is movement towards the electric vehicles is boosting demand for high-performance batteries with long-range and fast-charging capabilities. ⁤⁤In renewable energy, these batteries are crucial for storing solar and wind energy, making it easier to integrate them into the power grid and improving reliability. ⁤⁤What's more, lithium-ion batteries are versatile and can be used in various applications, from portable electronics to stationary energy storage and even aerospace. ⁤</p>
  
<img src={% static 'portfolio/lithium_usage.png' %} alt="api-logo" width="500" height="500">
      <br>    

<p>In this project, we've learned some interesting patterns about lithium-ion batteries and how different battery groups in our data are connected. This understanding will help us make our data better before we analyze it. We also learned some practical challenges in machine learning. For example, we found out that decision trees and naive Bayes models don't work well when our data is unbalanced. But the SVM model did a good job without us having to do extra effort like SMOTE, a method used to balance the class distribution. </p>
  <br>
<img src={% static 'portfolio/eda/3.png' %} alt="api-logo" width="941" height="263">
<br>
<p>In our experiment, battery groups 5 and 6 showed a big range of values. This shows they performed very differently in different tests. Maybe it's because of different conditions or changes in how the batteries were made. This shows how important it is to do experiments carefully and consistently to get good results. By understanding how these factors affect battery performance, we can make better conclusions about how batteries work. Instead of getting worse over time, they actually got better between tests. This is surprising and makes us wonder why it happened. Maybe it's because of how the batteries were treated, or the conditions they were in. By digging deeper into this, we can learn more about how batteries behave using after a resting interval and maybe find new ways to make them last longer. </p>

<p>When we tried to classify battery life by discretizing the battery life feature, the clustering methods suggested about 11 different categories. But when we looked closely, some of these categories were the same or overlapped. By combining insights from clustering and histogram based discretization results, we can make better classifications that show the true patterns in the data with only just 3 classes. 

We also looked at recent news about batteries. It seems like there's a lot of interest in making batteries better for things like virtual reality headsets, smartphones, and smartwatches. This tells us where researchers are focusing their efforts and gives us ideas for future research.</p>

<p>Lastly, we found that lower temperatures make batteries age slower, but there are many things to be explored still as it is a complex system, just looking at how much power is left in the battery isn't enough to tell us how much longer it will last. This shows that we need to consider lots of information regarding the charge cycle when we're thinking about how batteries work. By understanding all the factors that affect batteries life, we can make a predictive model which can be deployed in all devices which shows the battery life with an accuracy above 90%.</p>

<h3>Future Scope</h3>
<br>

<p>The future scope of this project is the integration of advanced predictive modeling techniques, particularly neural networks, to forecast battery cycles based on existing parameters. By expanding the analytical toolkit to include complex algorithms, such as deep learning models, we can unlock deeper insights into battery behavior and performance. Neural networks offer the ability to capture complex relationships and nonlinear dependencies within the data, enabling more accurate and robust predictions of battery cycles.Moreover,it opens up opportunities for real-time monitoring and adaptive management of battery systems. By deploying predictive models in sensor networks, we can continuously monitor battery health and performance, anticipate potential failures or degradation, and implement proactive maintenance strategies. This proactive approach to battery management can minimize downtime, optimize usage patterns, and extend battery lifespan, ultimately leading to more reliable and efficient energy storage solutions.</p>




</section>
</article>
{% endblock %}
